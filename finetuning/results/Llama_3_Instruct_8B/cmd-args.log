/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py exp_manager.exp_dir=./results/Meta-llama3.1-8B-Instruct-titlegen exp_manager.explicit_log_dir=./results/Meta-llama3.1-8B-Instruct-titlegen trainer.devices=1 trainer.num_nodes=1 trainer.precision=bf16-mixed trainer.val_check_interval=0.2 trainer.max_steps=50 model.megatron_amp_O2=True ++model.mcore_gpt=True model.tensor_model_parallel_size=1 model.pipeline_model_parallel_size=1 model.micro_batch_size=1 model.global_batch_size=32 model.restore_from_path=./llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo model.data.train_ds.file_names=[./curated-data/law-qa-train_preprocessed.jsonl] model.data.train_ds.concat_sampling_probabilities=[1.0] model.data.validation_ds.file_names=[./curated-data/law-qa-val_preprocessed.jsonl] model.peft.peft_scheme=lora